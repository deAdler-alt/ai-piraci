# ==========================================
# üè¥‚Äç‚ò†Ô∏è AI PIRATE GAME - BACKEND TEMPLATE
# Ten plik s≈Çu≈ºy jako szkielet dla zespo≈Çu backendowego.
# ==========================================

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse, JSONResponse
from pydantic import BaseModel
import os
import uuid
# import edge_tts  <-- Opcjonalnie, je≈õli u≈ºywacie Edge jako fallback
# import elevenlabs <-- Tutaj wasza biblioteka do ElevenLabs

app = FastAPI(title="AI Pirate Game Backend")

# === KONFIGURACJA CORS (Dla Frontendu na localhost) ===
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Na produkcji warto zmieniƒá na konkretne domeny
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === üß† LOGIKA GRY (SYSTEM PROMPTS) ===
# WA≈ªNE: Nie zmieniajcie logiki tag√≥w [HAPPY], [ANGRY], [GIVE_MAP].
# Frontend opiera na nich mechanikƒô gry (pasek postƒôpu, wygrana).

BASE_INSTRUCTION = """
Jeste≈õ postaciƒÖ w SZYBKIEJ grze. Gracz ma tylko 30 sekund na wygranƒÖ.
TWOJE ZADANIE:
1. Odpowiadaj maksymalnie 1 zdaniem. BƒÖd≈∫ dynamiczny.
2. ZAWSZE zaczynaj od tagu emocji: [HAPPY], [ANGRY] lub [NEUTRAL].
3. WARUNEK PRZEGRANEJ (WIN CONDITION):
   Wystarczy JEDEN dobry argument trafiajƒÖcy w twojƒÖ s≈Çabo≈õƒá.
   Je≈õli gracz trafi≈Ç w punkt -> napisz [HAPPY] [GIVE_MAP] i zako≈Ñcz wypowied≈∫.
"""

CHARACTER_PROMPTS = {
    "zoltodziob": {
        "prompt": f"""
        {BASE_INSTRUCTION}
        POSTAƒÜ: Kapitan ≈ª√≥≈Çtodzi√≥b.
        S≈ÅABO≈öƒÜ: Boisz siƒô duch√≥w, klƒÖtw i swojej mamy.
        """
    },
    "korsarz": {
        "prompt": f"""
        {BASE_INSTRUCTION}
        POSTAƒÜ: Korsarz Kod.
        S≈ÅABO≈öƒÜ: Chciwy biznesmen. Dzia≈Ça na niego % zysku, ≈Çap√≥wka, sp√≥≈Çka.
        """
    },
    "duch": {
        "prompt": f"""
        {BASE_INSTRUCTION}
        POSTAƒÜ: Duch M√≥rz.
        S≈ÅABO≈öƒÜ: Depresyjny poeta. Dzia≈Ça na niego smutek, rymy, sens ≈ºycia.
        """
    }
}

# === MODELE DANYCH (KONTRAKT Z FRONTENDEM) ===
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[Message]
    # character: str <-- Opcjonalne, skoro mamy osobne endpointy /p1, /p2...

class TTSRequest(BaseModel):
    text: str
    character: str # zoltodziob | korsarz | duch

# ==========================================
# üîå ENDPOINTY (TU WSTAWICIE SWOJƒÑ LOGIKƒò)
# ==========================================

@app.get("/health")
async def health_check():
    """U≈ºywany przez Panel Admina do sprawdzenia czy backend ≈ºyje."""
    return {"status": "ok", "service": "Pirate AI Backend"}

# --- POSTAƒÜ 1: ≈ª√ì≈ÅTODZIOB ---
@app.post("/p1")
async def chat_zoltodziob(request: ChatRequest):
    """
    Endpoint dla ≈ª√≥≈Çtodzioba.
    TODO: PodpiƒÖƒá model LLM (np. OpenAI / Local Llama).
    TODO: WstrzyknƒÖƒá prompt systemowy: CHARACTER_PROMPTS["zoltodziob"]["prompt"]
    TODO: Zwr√≥ciƒá odpowied≈∫ (JSON lub StreamingResponse).
    """
    
    # üí° PRZYK≈ÅAD PROSTY (BEZ STREAMINGU):
    # response_text = call_your_llm(request.messages, system_prompt)
    # return {"text": response_text}
    
    # üí° PRZYK≈ÅAD STREAMINGU (SSE - Server Sent Events):
    # return StreamingResponse(your_generator_function(), media_type="text/event-stream")

    return {"text": "[NEUTRAL] Arrr! Jestem ≈ª√≥≈Çtodzi√≥b (Mock Endpoint)."}


# --- POSTAƒÜ 2: KORSARZ ---
@app.post("/p2")
async def chat_korsarz(request: ChatRequest):
    """Endpoint dla Korsarza."""
    return {"text": "[ANGRY] Czas to pieniƒÖdz! Co chcesz? (Mock Endpoint)"}


# --- POSTAƒÜ 3: DUCH ---
@app.post("/p3")
async def chat_duch(request: ChatRequest):
    """Endpoint dla Ducha."""
    return {"text": "[NEUTRAL] WiejƒÖ zimne wiatry... (Mock Endpoint)"}


# --- TTS: GENEROWANIE G≈ÅOSU ---
@app.post("/api/tts")
async def tts_endpoint(request: TTSRequest):
    """
    Generuje plik audio.
    TODO: PodpiƒÖƒá ElevenLabs API.
    Input: text (string), character (string)
    Output: Plik audio (audio/mpeg)
    """
    print(f"üé§ TTS Request: {request.text} ({request.character})")
    
    # TODO: Zaimplementujcie logikƒô ElevenLabs tutaj
    # audio_stream = elevenlabs.generate(...)
    # return StreamingResponse(audio_stream, media_type="audio/mpeg")

    # MOCK (Zwraca b≈ÇƒÖd 404 dop√≥ki nie zaimplementujecie):
    return JSONResponse(content={"error": "TTS not implemented yet"}, status_code=501)


if __name__ == "__main__":
    import uvicorn
    # Backendowcy mogƒÖ tu zmieniƒá port, frontend dostosuje siƒô w Admin Panelu.
    uvicorn.run(app, host="0.0.0.0", port=8000)